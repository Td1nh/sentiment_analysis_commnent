import pandas as pd
import streamlit as st
from underthesea import word_tokenize, pos_tag, sent_tokenize
import regex
import re
import joblib
from underthesea import word_tokenize, pos_tag, sent_tokenize
from scipy.sparse import hstack


# ================================= DATA Sá»¬ Dá»¤NG CHO HÃ€M ===================================
#LOAD EMOJICON
file = open('files/emojicon.txt', 'r', encoding="utf8")
emoji_lst = file.read().split('\n')
emoji_dict = {}
for line in emoji_lst:
    key, value = line.split('\t')
    emoji_dict[key] = str(value)
file.close()

#LOAD TEENCODE & ABBREVIATION
file = open('files/teencode.txt', 'r', encoding="utf8")
teen_lst = file.read().split('\n')
teen_dict = {}
for line in teen_lst:
    key, value = line.split('\t')
    teen_dict[key] = str(value)
file.close()

#LOAD TRANSLATE ENGLISH -> VNMESE
file = open('files/english-vnmese.txt', 'r', encoding="utf8")
english_lst = file.read().split('\n')
english_dict = {}
for line in english_lst:
    key, value = line.split('\t')
    english_dict[key] = str(value)
file.close()

#LOAD wrong words
file = open('files/wrong-word.txt', 'r', encoding="utf8")
wrong_lst = file.read().split('\n')
file.close()

#LOAD STOPWORDS
file = open('files/vietnamese-stopwords.txt', 'r', encoding="utf8")
stopwords_lst = file.read().split('\n')
file.close()

#LOAD POSITIVE
file = open('files/positive_VN.txt', 'r', encoding="utf8")
positive_words = file.read().split('\n')
file.close()

#LOAD NEGATIVE
file = open('files/negative_VN.txt', 'r', encoding="utf8")
negative_words = file.read().split('\n')
file.close()

#LOAD POSITIVE EMO
file = open('files/positive_emoji.txt', 'r', encoding="utf8")
positive_emojis = file.read().split('\n')
file.close()

#LOAD NEGATIVE EMO
file = open('files/negative_emoji.txt', 'r', encoding="utf8")
negative_emojis = file.read().split('\n')
file.close()

# ================================== FUNCTION LIÃŠN QUAN ===================================
# ThÃªm dáº¥u cÃ¡ch trÆ°á»›c emoji náº¿u chÆ°a cÃ³
def add_emoji_spaces(text):
        # Find and add spaces around emojis
        for emoji in emoji_dict.keys():
            # Add space before and after emoji
            text = text.replace(emoji, f' {emoji} ')
        return text

# Sá»¬A EMOJI, TEENCODE, SAI CHÃNH Táº¢, Bá» KÃ Tá»° Äáº¶C BIá»†T, TIáº¾NG ANH SANG TIáº¾NG VIá»†T
def process_text(text, emoji_dict, teen_dict, english_dict):
    # Apply emoji space handling first
    document = add_emoji_spaces(text.lower())
    document = document.replace("'",'')
    document = regex.sub(r'\.+', ".", document)
    new_sentence = ''
    for sentence in sent_tokenize(document):
        # CONVERT EMOJICON
        sentence = ''.join(emoji_dict[word]+' ' if word in emoji_dict else word for word in list(sentence))

        # CONVERT TEENCODE & ABBREVIATION
        sentence = ' '.join(teen_dict[word] if word in teen_dict else word for word in sentence.split())

        # DEL Punctuation & Numbers
        pattern = r'(?i)\b[a-zÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘]+\b'
        sentence = ' '.join(regex.findall(pattern,sentence))

        # TRANSFORM ENGLISH TO VIETNAMESE
        sentence = ' '.join(english_dict[word] if word in english_dict else word for word in sentence.split())
        new_sentence = new_sentence + sentence + '. '

    document = new_sentence
    # DEL excess blank space
    document = regex.sub(r'\s+', ' ', document).strip()
    return document

# CHUáº¨N HÃ“A UNICODE TIáº¾NG VIá»†T
def loaddicchar():
    uniChars = "Ã Ã¡áº£Ã£áº¡Ã¢áº§áº¥áº©áº«áº­Äƒáº±áº¯áº³áºµáº·Ã¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡Ä‘Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±á»³Ã½á»·á»¹á»µÃ€Ãáº¢Ãƒáº Ã‚áº¦áº¤áº¨áºªáº¬Ä‚áº°áº®áº²áº´áº¶ÃˆÃ‰áººáº¼áº¸ÃŠá»€áº¾á»‚á»„á»†ÄÃŒÃá»ˆÄ¨á»ŠÃ’Ã“á»Ã•á»ŒÃ”á»’á»á»”á»–á»˜Æ á»œá»šá»á» á»¢Ã™Ãšá»¦Å¨á»¤Æ¯á»ªá»¨á»¬á»®á»°á»²Ãá»¶á»¸á»´Ã‚Ä‚ÄÃ”Æ Æ¯"
    unsignChars = "aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU"
    dic = {}
    char1252 = 'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£'.split(
        '|')
    charutf8 = "Ã |Ã¡|áº£|Ã£|áº¡|áº§|áº¥|áº©|áº«|áº­|áº±|áº¯|áº³|áºµ|áº·|Ã¨|Ã©|áº»|áº½|áº¹|á»|áº¿|á»ƒ|á»…|á»‡|Ã¬|Ã­|á»‰|Ä©|á»‹|Ã²|Ã³|á»|Ãµ|á»|á»“|á»‘|á»•|á»—|á»™|á»|á»›|á»Ÿ|á»¡|á»£|Ã¹|Ãº|á»§|Å©|á»¥|á»«|á»©|á»­|á»¯|á»±|á»³|Ã½|á»·|á»¹|á»µ|Ã€|Ã|áº¢|Ãƒ|áº |áº¦|áº¤|áº¨|áºª|áº¬|áº°|áº®|áº²|áº´|áº¶|Ãˆ|Ã‰|áºº|áº¼|áº¸|á»€|áº¾|á»‚|á»„|á»†|ÃŒ|Ã|á»ˆ|Ä¨|á»Š|Ã’|Ã“|á»|Ã•|á»Œ|á»’|á»|á»”|á»–|á»˜|á»œ|á»š|á»|á» |á»¢|Ã™|Ãš|á»¦|Å¨|á»¤|á»ª|á»¨|á»¬|á»®|á»°|á»²|Ã|á»¶|á»¸|á»´".split(
        '|')
    for i in range(len(char1252)):
        dic[char1252[i]] = charutf8[i]
    return dic

def covert_unicode(txt):
    dicchar = loaddicchar()
    return regex.sub(
        r'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£',
        lambda x: dicchar[x.group()], txt)

# Ná»I CÃC Tá»ª Cáº¦N Xá»¬ LÃ Äáº¶C BIá»†T
def process_special_word(text):
    if not text:
        return ""
    # Danh sÃ¡ch cÃ¡c tá»« Ä‘áº·c biá»‡t cáº§n xá»­ lÃ½
    special_words = {'khÃ´ng', 'cháº£', 'kÃ©m', 'cháº³ng', 'Ä‘á»«ng', 'chá»›', 'chÆ°a', 'khÃ´ng_cÃ³'}
    words = text.split()
    result = []
    i = 0
    while i < len(words):
        current_word = words[i]

        if current_word in special_words and i + 1 < len(words):
            # Ná»‘i tá»« Ä‘áº·c biá»‡t vá»›i tá»« tiáº¿p theo
            combined_word = f"{current_word}_{words[i + 1]}"
            result.append(combined_word)
            i += 2  # Bá» qua tá»« tiáº¿p theo vÃ¬ Ä‘Ã£ xá»­ lÃ½
        else:
            result.append(current_word)
            i += 1

    return ' '.join(result)

# CHUáº¨N HÃ“A Tá»ª CÃ“ KÃ Tá»° Láº¶P
def normalize_repeated_characters(text):
    return re.sub(r'(.)\1+', r'\1', text)

def process_postag_thesea(text):
    # TÃ¡ch cÃ¡c biá»ƒu tÆ°á»£ng (icons) khá»i vÄƒn báº£n
    icons = regex.findall(r'[\U00010000-\U0010FFFF]+', text)  # TÃ¬m cÃ¡c kÃ½ tá»± biá»ƒu tÆ°á»£ng Unicode
    icons_text = ' '.join(icons)

    # Loáº¡i bá» cÃ¡c biá»ƒu tÆ°á»£ng khá»i vÄƒn báº£n gá»‘c
    text_without_icons = regex.sub(r'[\U00010000-\U0010FFFF]+', '', text)

    new_document = ''
    for sentence in sent_tokenize(text_without_icons):
        sentence = sentence.replace('.', '')
        ###### POS tag
        lst_word_type = ['N','Np','A','AB','V','VB','VY','R', 'AA']
        sentence = ' '.join(word[0] if word[1].upper() in lst_word_type else '' for word in pos_tag(word_tokenize(sentence, format="text")))
        new_document = new_document + sentence + ' '

    ###### DEL excess blank space
    new_document = regex.sub(r'\s+', ' ', new_document).strip()
    # GhÃ©p láº¡i cÃ¡c icon Ä‘Ã£ tÃ¡ch vÃ o cuá»‘i vÄƒn báº£n xá»­ lÃ½
    final_document = new_document + ' ' + icons_text
    return final_document

# REMOVE CÃC Tá»ª SAI VÃ€ CÃC STOPWORD
def remove_stopword(text, stopwords, wrong_lst):
    ###### REMOVE stop words
    document = ' '.join('' if word in stopwords else word for word in text.split())
    ###### DEL wrong words
    document = ' '.join('' if word in wrong_lst else word for word in document.split())
    ###### DEL excess blank space
    document = regex.sub(r'\s+', ' ', document).strip()
    return document

class VietnameseTextProcessor:
    def __init__(self, emoji_dict, teen_dict, wrong_lst, english_dict, stopwords_lst):
        self.emoji_dict = emoji_dict
        self.teen_dict = teen_dict
        self.wrong_lst = wrong_lst
        self.english_dict = english_dict
        self.stopwords_lst = stopwords_lst

    def process_pipeline(self, text):
        # Chuyá»ƒn text vá» string náº¿u khÃ´ng pháº£i
        text = str(text)
        # 1. Xá»­ lÃ½ emoji, teencode, dáº¥u cÃ¢u, sá»‘ vÃ  chuyá»ƒn Ä‘á»•i tiáº¿ng Anh
        text = process_text(
            text,
            self.emoji_dict,
            self.teen_dict,
            self.english_dict,
        )
        # 2. Chuáº©n hÃ³a unicode
        text = covert_unicode(text)
        # 3. Chuáº©n hÃ³a kÃ½ tá»± láº·p
        text = normalize_repeated_characters(text)
        # 4. Gá»™p cÃ¡c tá»« ghÃ©p
        text = process_postag_thesea(text)
        # 5. Xá»­ lÃ½ cÃ¡c tá»« Ä‘áº·c biá»‡t
        text = process_special_word(text)
        # 6. Loáº¡i bá» stopwords vÃ  tá»« sai
        text = remove_stopword(text, self.stopwords_lst, self.wrong_lst)
        return text

def find_words(text, word_list):
    text_lower = text.lower()
    word_count = 0
    found_words = []
    # TÃ¡ch cÃ¡c tá»« trong vÄƒn báº£n báº±ng dáº¥u cÃ¡ch
    words_in_text = text_lower.split()
    for text_word in words_in_text:
        if text_word in word_list:
          found_words.append(text_word)
          word_count += 1
    return word_count, found_words

def sentiment_pipeline(document, positive_words, negative_words, positive_emojis, negative_emojis):
    # Calculate word sentiment
    positive_count, positive_word_list = find_words(document, positive_words)
    negative_count, negative_word_list = find_words(document, negative_words)

    # Calculate emoji sentiment
    positive_icon, positive_icon_list = find_words(document, positive_emojis)
    negative_icon, negative_icon_list = find_words(document, negative_emojis)

    # Total sentiment calculation
    total_positive = positive_count + positive_icon
    total_negative = negative_count + negative_icon
    return total_positive, positive_word_list + positive_icon_list, total_negative, negative_word_list + negative_icon_list


def preprocess_sentiment_text(text, processor, positive_words, negative_words, positive_emojis, negative_emojis):
    # Tiá»n xá»­ lÃ½ vÄƒn báº£n
    processed_text = processor.process_pipeline(text)
    text1 = process_special_word(process_postag_thesea(normalize_repeated_characters(add_emoji_spaces(text))))
    # Dá»± Ä‘oÃ¡n cáº£m xÃºc
    result = sentiment_pipeline(text1, positive_words, negative_words, positive_emojis, negative_emojis)
    # Táº¡o DataFrame chá»©a káº¿t quáº£
    data = {
        "noi_dung_binh_luan": [text],
        "noi_dung_binh_luan_processed": [processed_text],
        "positive_count": [result[0]],
        "negative_count": [result[2]]
    }
    du_doan = pd.DataFrame(data)
    du_doan['do_dai'] = du_doan['noi_dung_binh_luan'].apply(lambda x: len(str(x).split()))
    return du_doan


def x_with_tfidf_model(du_doan, model_path='saved_models/tfidf_model.pkl'):
    # Táº£i mÃ´ hÃ¬nh TF-IDF vÃ  vectorizer tá»« file
    train, tfidf_vectorizer = joblib.load(model_path)
    # Chuyá»ƒn vÄƒn báº£n Ä‘Ã£ xá»­ lÃ½ thÃ nh vector TF-IDF
    du_doan_vec = tfidf_vectorizer.transform(du_doan['noi_dung_binh_luan_processed'])
    # Káº¿t há»£p cÃ¡c Ä‘áº·c trÆ°ng sá»‘
    du_doan_num = du_doan[['do_dai', 'positive_count', 'negative_count']].values
    du_doan_combined = hstack([du_doan_vec, du_doan_num])
    return du_doan_combined

# ================================== DATA ===================================
# Äá»c data Ä‘Ã¡nh giÃ¡
danh_gia = pd.read_csv('data/DATA - FINAL/Danh_gia_clean.csv', sep=';')

# ================================== STREAMLIT ===================================
st.set_page_config(
    page_title="Dá»± Ä‘oÃ¡n nhÃ£n",
    page_icon="ğŸ“‰",
    layout = "wide"
)

st.sidebar.write("""#### ThÃ nh viÃªn thá»±c hiá»‡n:\n
                 Trang ThÆ° ÄÃ¬nh
                 Nguyá»…n Quang Kháº£i""")
st.sidebar.write("""#### Giáº£ng viÃªn hÆ°á»›ng dáº«n:\n
                Khuáº¥t ThÃ¹y PhÆ°Æ¡ng""")
st.sidebar.write("""#### Thá»i gian thá»±c hiá»‡n: 7/12/2024""")

st.subheader("Dá»± Ä‘oÃ¡n nhÃ£n")
processor = VietnameseTextProcessor(
    emoji_dict=emoji_dict,
    teen_dict=teen_dict,
    wrong_lst=wrong_lst,
    english_dict=english_dict,
    stopwords_lst=stopwords_lst
)
flag = False
lines = None
type = st.radio("Táº£i bÃ¬nh lÃªn hay Nháº­p liá»‡u?", options=("Táº£i lÃªn", "Nháº­p bÃ¬nh luáº­n"))
if type=="Táº£i lÃªn":
    # Upload file
    uploaded_file_1 = st.file_uploader("Chá»n file", type=['txt', 'csv', 'xlsx'])
    if uploaded_file_1 is not None:
        # Kiá»ƒm tra loáº¡i file vÃ  Ä‘á»c tÆ°Æ¡ng á»©ng
        if uploaded_file_1.name.endswith('.csv') or uploaded_file_1.name.endswith('.txt'):
            # Äá»c file CSV hoáº·c TXT, bá» qua cÃ¡c dÃ²ng cÃ³ lá»—i
            lines = pd.read_csv(uploaded_file_1, header=None, sep=';')
        elif uploaded_file_1.name.endswith('.xlsx'):
            # Äá»c file Excel
            lines = pd.read_excel(uploaded_file_1, header=None)
        # Chá»‰ láº¥y cá»™t Ä‘áº§u tiÃªn
        lines = lines[0]  
        # Táº¡o DataFrame má»›i vÃ  Ä‘á»•i tÃªn cá»™t thÃ nh 'noi_dung_binh_luan'
        du_doan = pd.DataFrame(lines)
        du_doan.columns = ['noi_dung_binh_luan']
        # Hiá»ƒn thá»‹ DataFrame má»›i
        st.write(f"Ná»™i dung bÃ¬nh luáº­n:")
        st.dataframe(du_doan)

        st.write('Äang xá»­ lÃ½ dá»¯ liá»‡u...')
        # LÆ°u Ã½: Cáº§n cung cáº¥p cÃ¡c tham sá»‘ nhÆ° processor, positive_words, negative_words, positive_emojis, negative_emojis.
        df_processed = du_doan['noi_dung_binh_luan'].apply(
            lambda x: preprocess_sentiment_text(x, processor, positive_words, negative_words, positive_emojis, negative_emojis)
        )
        # Merging káº¿t quáº£ vÃ o má»™t DataFrame duy nháº¥t
        du_doan = pd.concat(df_processed.tolist(), ignore_index=True)

        du_doan_combined = x_with_tfidf_model(du_doan, model_path='saved_models/tfidf_model.pkl')
        
        # Táº£i mÃ´ hÃ¬nh
        st.write(f"Äang táº£i mÃ´ hÃ¬nh...")
        loaded_model = joblib.load('saved_models/Random_Forest_Classifier.pkl', mmap_mode='r')

        # Dá»± Ä‘oÃ¡n nhÃ£n
        predictions = loaded_model.predict(du_doan_combined)
        st.write(f"Dá»± Ä‘oÃ¡n lÃ  nhÃ£n: {predictions}")

        # Dá»± Ä‘oÃ¡n xÃ¡c suáº¥t
        probabilities = loaded_model.predict_proba(du_doan_combined)

        # In xÃ¡c suáº¥t theo tá»«ng máº«u
        st.write(f"XÃ¡c xuáº¥t cá»§a cÃ¡c nhÃ£n:\n")
        class_labels = loaded_model.classes_
        prob_df = pd.DataFrame(probabilities, columns=class_labels)
        result = pd.merge(du_doan['noi_dung_binh_luan'], prob_df, left_index=True, right_index=True)
        def highlight_max_in_row(row):
            # So sÃ¡nh tá»« cá»™t thá»© 2 trá»Ÿ Ä‘i vÃ  Ã¡p dá»¥ng mÃ u vÃ ng cho giÃ¡ trá»‹ lá»›n nháº¥t
            return ['background-color: yellow' if v == row[1:].max() else '' for v in row]
        st.dataframe(result.style.apply(highlight_max_in_row, axis=1))


if type=="Nháº­p bÃ¬nh luáº­n":        
    text = st.text_area(label="Input your content:")
    if text!="":
        flag = True

        st.write('Äang xá»­ lÃ½ dá»¯ liá»‡u...')
        du_doan = preprocess_sentiment_text(text, processor, positive_words, negative_words, positive_emojis, negative_emojis)
        du_doan_combined = x_with_tfidf_model(du_doan, model_path='saved_models/tfidf_model.pkl')

        # Táº£i mÃ´ hÃ¬nh
        st.write(f"Äang táº£i mÃ´ hÃ¬nh...")
        loaded_model = joblib.load('saved_models/Random_Forest_Classifier.pkl', mmap_mode='r')

        st.write(f"Ná»™i dung bÃ¬nh luáº­n: {text}")

        # Dá»± Ä‘oÃ¡n nhÃ£n
        predictions = loaded_model.predict(du_doan_combined)
        st.write(f"Dá»± Ä‘oÃ¡n lÃ  nhÃ£n: {predictions}")

        # Dá»± Ä‘oÃ¡n xÃ¡c suáº¥t
        probabilities = loaded_model.predict_proba(du_doan_combined)

        # In xÃ¡c suáº¥t theo tá»«ng máº«u
        st.write(f"XÃ¡c xuáº¥t cá»§a cÃ¡c nhÃ£n:\n")
        class_labels = loaded_model.classes_
        prob_df = pd.DataFrame(probabilities, columns=class_labels)
        def highlight_max_in_row(row):
            return ['background-color: yellow' if v == row.max() else '' for v in row]
        styled_df = prob_df.style.apply(highlight_max_in_row, axis=1)
        st.dataframe(styled_df)




    