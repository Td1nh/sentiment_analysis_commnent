import pandas as pd
import streamlit as st
from underthesea import word_tokenize, pos_tag, sent_tokenize
import regex
import re
import joblib
from scipy.sparse import hstack
from pyvi import ViPosTagger, ViTokenizer


# ================================= DATA Sá»¬ Dá»¤NG CHO HÃ€M ===================================
#################
#LOAD EMOJICON
file = open('files/emojicon.txt', 'r', encoding="utf8")
emoji_lst = file.read().split('\n')
emoji_dict = {}
for line in emoji_lst:
    key, value = line.split('\t')
    emoji_dict[key] = str(value)
file.close()

#################
#LOAD TEENCODE & ABBREVIATION
file = open('files/teencode.txt', 'r', encoding="utf8")
teen_lst = file.read().split('\n')
teen_dict = {}
for line in teen_lst:
    key, value = line.split('\t')
    teen_dict[key] = str(value)
file.close()

#################
#LOAD STOPWORDS
file = open('files/vietnamese-stopwords.txt', 'r', encoding="utf8")
stopwords_lst = file.read().split('\n')
file.close()

#LOAD CORRECTION
file = open('files/corrected_word.txt', 'r', encoding="utf8")
corrected_word_lst = file.read().split('\n')
corrected_word_dict = {}
for line in corrected_word_lst:
    key, value = line.split('\t')
    corrected_word_dict[key] = str(value)
file.close()

#LOAD positive words
file = open('files/positive_VN.txt', 'r', encoding="utf8")
positive_words = file.read().split('\n')
file.close()

#LOAD negative words
file = open('files/negative_VN.txt', 'r', encoding="utf8")
negative_words = file.read().split('\n')
file.close()

#LOAD positive emojis
file = open('files/positive_emoji.txt', 'r', encoding="utf8")
positive_emojis = file.read().split('\n')
file.close()

#LOAD positive emojis
file = open('files/negative_emoji.txt', 'r', encoding="utf8")
negative_emojis = file.read().split('\n')
file.close()

# ================================== FUNCTION LIÃŠN QUAN ===================================
# THÃŠM Dáº¤U CÃCH TRÆ¯á»šC EMOJI
def add_emoji_spaces(text):
        # Find and add spaces around emojis
        for emoji in emoji_dict.keys():
            # Add space before and after emoji
            text = text.replace(emoji, f' {emoji} ')
        return text



# Sá»¬A EMOJI, TEENCODE, SAI CHÃNH Táº¢, Bá» KÃ Tá»° Äáº¶C BIá»†T
def process_text(text, emoji_dict, teen_dict):
    # Apply emoji space handling first
    document = add_emoji_spaces(text.lower())
    document = document.replace("'",'')
    document = regex.sub(r'\.+', ".", document)
    new_sentence = ''
    for sentence in sent_tokenize(document):
        # CONVERT EMOJICON
        sentence = ''.join(emoji_dict[word]+' ' if word in emoji_dict else word for word in list(sentence))

        # CONVERT TEENCODE & ABBREVIATION
        sentence = ' '.join(teen_dict[word] if word in teen_dict else word for word in sentence.split())

        # DEL Punctuation & Numbers
        pattern = r'(?i)\b[a-zÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘]+\b'
        sentence = ' '.join(regex.findall(pattern,sentence))

        new_sentence = new_sentence + sentence + '. '
    document = new_sentence
    # DEL excess blank space
    document = regex.sub(r'\s+', ' ', document).strip()
    return document



# CHUáº¨N HÃ“A UNICODE TIáº¾NG VIá»†T
def loaddicchar():
    uniChars = "Ã Ã¡áº£Ã£áº¡Ã¢áº§áº¥áº©áº«áº­Äƒáº±áº¯áº³áºµáº·Ã¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡Ä‘Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±á»³Ã½á»·á»¹á»µÃ€Ãáº¢Ãƒáº Ã‚áº¦áº¤áº¨áºªáº¬Ä‚áº°áº®áº²áº´áº¶ÃˆÃ‰áººáº¼áº¸ÃŠá»€áº¾á»‚á»„á»†ÄÃŒÃá»ˆÄ¨á»ŠÃ’Ã“á»Ã•á»ŒÃ”á»’á»á»”á»–á»˜Æ á»œá»šá»á» á»¢Ã™Ãšá»¦Å¨á»¤Æ¯á»ªá»¨á»¬á»®á»°á»²Ãá»¶á»¸á»´Ã‚Ä‚ÄÃ”Æ Æ¯"
    unsignChars = "aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU"
    dic = {}
    char1252 = 'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£'.split(
        '|')
    charutf8 = "Ã |Ã¡|áº£|Ã£|áº¡|áº§|áº¥|áº©|áº«|áº­|áº±|áº¯|áº³|áºµ|áº·|Ã¨|Ã©|áº»|áº½|áº¹|á»|áº¿|á»ƒ|á»…|á»‡|Ã¬|Ã­|á»‰|Ä©|á»‹|Ã²|Ã³|á»|Ãµ|á»|á»“|á»‘|á»•|á»—|á»™|á»|á»›|á»Ÿ|á»¡|á»£|Ã¹|Ãº|á»§|Å©|á»¥|á»«|á»©|á»­|á»¯|á»±|á»³|Ã½|á»·|á»¹|á»µ|Ã€|Ã|áº¢|Ãƒ|áº |áº¦|áº¤|áº¨|áºª|áº¬|áº°|áº®|áº²|áº´|áº¶|Ãˆ|Ã‰|áºº|áº¼|áº¸|á»€|áº¾|á»‚|á»„|á»†|ÃŒ|Ã|á»ˆ|Ä¨|á»Š|Ã’|Ã“|á»|Ã•|á»Œ|á»’|á»|á»”|á»–|á»˜|á»œ|á»š|á»|á» |á»¢|Ã™|Ãš|á»¦|Å¨|á»¤|á»ª|á»¨|á»¬|á»®|á»°|á»²|Ã|á»¶|á»¸|á»´".split(
        '|')
    for i in range(len(char1252)):
        dic[char1252[i]] = charutf8[i]
    return dic

def covert_unicode(txt):
    dicchar = loaddicchar()
    return regex.sub(
        r'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£',
        lambda x: dicchar[x.group()], txt)



# Ná»I CÃC Tá»ª Cáº¦N Xá»¬ LÃ Äáº¶C BIá»†T
def process_special_word(text):
    if not text:
        return ""
    # Danh sÃ¡ch cÃ¡c tá»« Ä‘áº·c biá»‡t cáº§n xá»­ lÃ½
    special_words = {'khÃ´ng', 'cháº£', 'kÃ©m', 'cháº³ng', 'Ä‘á»«ng', 'chá»›', 'chÆ°a', 'khÃ´ng_cÃ³', 'khÃ´ng_quÃ¡'}
    words = text.split()
    result = []
    i = 0
    while i < len(words):
        current_word = words[i]

        if current_word in special_words and i + 1 < len(words):
            # Ná»‘i tá»« Ä‘áº·c biá»‡t vá»›i tá»« tiáº¿p theo
            combined_word = f"{current_word}_{words[i + 1]}"
            result.append(combined_word)
            i += 2  # Bá» qua tá»« tiáº¿p theo vÃ¬ Ä‘Ã£ xá»­ lÃ½
        else:
            result.append(current_word)
            i += 1
    return ' '.join(result)

# CHUáº¨N HÃ“A Tá»ª CÃ“ KÃ Tá»° Láº¶P
def normalize_repeated_characters(text):
    return re.sub(r'(.)\1+', r'\1', text)



# Ná»I CÃC Tá»ª GHÃ‰P
def process_postag_pyvi(text):
    # TÃ¡ch cÃ¢u
    sentences = text.split('.')  # Chia cÃ¡c cÃ¢u dá»±a trÃªn dáº¥u cháº¥m.
    lst_word_type = ['A', 'V', 'R']
    processed_sentences = []
    for sentence in sentences:
        # Bá» khoáº£ng tráº¯ng thá»«a
        sentence = sentence.strip()
        if not sentence:
            continue
        # Tokenize vÃ  POS tagging
        tokens = ViTokenizer.tokenize(sentence)
        words, pos_tags = ViPosTagger.postagging(tokens)
        # Giá»¯ láº¡i cÃ¡c tá»« thuá»™c POS mong muá»‘n
        filtered_words = [word for word, pos in zip(words, pos_tags) if pos in lst_word_type]
        # GhÃ©p cÃ¡c tá»« láº¡i thÃ nh cÃ¢u
        processed_sentences.append(' '.join(filtered_words))
    # GhÃ©p láº¡i cÃ¡c cÃ¢u thÃ nh vÄƒn báº£n hoÃ n chá»‰nh
    result = ' '.join(processed_sentences).strip()
    return result



# LOáº I Bá» STOPWORD
def remove_stopword(text, stopwords):
    ###### REMOVE stop words
    document = ' '.join('' if word in stopwords else word for word in text.split())

    ###### DEL excess blank space
    document = regex.sub(r'\s+', ' ', document).strip()
    return document


# CHá»ˆNH Sá»¬A CHÃNH Táº¢
def correct_spelling(text, corrected_word_dict):
    # Táº¡o regex pattern Ä‘á»ƒ tÃ¬m cÃ¡c tá»« cáº§n thay tháº¿
    pattern = regex.compile(r'\b(' + '|'.join(regex.escape(key) for key in corrected_word_dict.keys()) + r')\b')
    # HÃ m thay tháº¿ tá»« sai thÃ nh tá»« Ä‘Ãºng
    def replace_word(match):
        word = match.group(0)
        return corrected_word_dict.get(word, word)
    # Thá»±c hiá»‡n thay tháº¿
    corrected_text = pattern.sub(replace_word, text)
    return corrected_text


# PIPELINE
class VietnameseTextProcessor:
    def __init__(self, emoji_dict, teen_dict, stopwords_lst):
        self.emoji_dict = emoji_dict
        self.teen_dict = teen_dict
        self.stopwords_lst = stopwords_lst

    def process_pipeline(self, text):
        # Chuyá»ƒn text vá» string náº¿u khÃ´ng pháº£i
        text = str(text)

        text = add_emoji_spaces(text)

        text = process_text(text, emoji_dict, teen_dict)

        text = correct_spelling(text, corrected_word_dict)

        text = covert_unicode(text)

        text = normalize_repeated_characters(text)

        text = process_postag_pyvi(text)

        text = process_special_word(text)
        
        text = remove_stopword(text, stopwords_lst)
        return text

# Khá»Ÿi táº¡o processor
processor = VietnameseTextProcessor(
    emoji_dict=emoji_dict,
    teen_dict=teen_dict,
    stopwords_lst=stopwords_lst
)


def find_words(text, word_list):
    text_lower = text.lower()
    word_count = 0
    found_words = []
    # TÃ¡ch cÃ¡c tá»« trong vÄƒn báº£n báº±ng dáº¥u cÃ¡ch
    words_in_text = text_lower.split()
    for text_word in words_in_text:
        if text_word in word_list:
          found_words.append(text_word)
          word_count += 1
    return word_count, found_words


def sentiment_pipeline(document, positive_words, negative_words, positive_emojis, negative_emojis):
    # Calculate word sentiment
    positive_count, positive_word_list = find_words(document, positive_words)
    negative_count, negative_word_list = find_words(document, negative_words)

    # Calculate emoji sentiment
    positive_icon, positive_icon_list = find_words(document, positive_emojis)
    negative_icon, negative_icon_list = find_words(document, negative_emojis)

    # Total sentiment calculation
    total_positive = positive_count + positive_icon
    total_negative = negative_count + negative_icon
    return total_positive, positive_word_list + positive_icon_list, total_negative, negative_word_list + negative_icon_list


def preprocess_sentiment_text(text, processor, positive_words, negative_words, positive_emojis, negative_emojis):
    # Tiá»n xá»­ lÃ½ vÄƒn báº£n
    processed_text = processor.process_pipeline(text)
    count_text = process_special_word(process_postag_pyvi(normalize_repeated_characters(add_emoji_spaces(text))))
    # Dá»± Ä‘oÃ¡n cáº£m xÃºc
    result = sentiment_pipeline(count_text, positive_words, negative_words, positive_emojis, negative_emojis)
    # Táº¡o DataFrame chá»©a káº¿t quáº£
    data = {
        "noi_dung_binh_luan": [text],
        "noi_dung_binh_luan_processed": [processed_text],
        "positive_count": [result[0]],
        "negative_count": [result[2]]
    }
    du_doan = pd.DataFrame(data)
    du_doan['do_dai'] = du_doan['noi_dung_binh_luan'].apply(lambda x: len(str(x).split()))
    return du_doan


def x_with_count_vectorizer_model(du_doan, model_path='saved_models/count_vectorizer_model.pkl'):
    # Táº£i mÃ´ hÃ¬nh vÃ  vectorizer tá»« file
    train, count_vectorizer = joblib.load(model_path)
    # Chuyá»ƒn vÄƒn báº£n Ä‘Ã£ xá»­ lÃ½ thÃ nh vector 
    du_doan_vec = count_vectorizer.transform(du_doan['noi_dung_binh_luan_processed'])
    # Káº¿t há»£p cÃ¡c Ä‘áº·c trÆ°ng sá»‘
    du_doan_num = du_doan[['do_dai', 'positive_count', 'negative_count']].values
    du_doan_combined = hstack([du_doan_vec, du_doan_num])
    return du_doan_combined

# ================================== DATA ===================================
# Äá»c data Ä‘Ã¡nh giÃ¡
danh_gia = pd.read_csv('data/DATA - FINAL/Danh_gia_clean.csv', sep=';')

# ================================== STREAMLIT ===================================
st.set_page_config(
    page_title="Dá»± Ä‘oÃ¡n nhÃ£n",
    page_icon="ğŸ“‰",
    layout = "wide"
)

st.sidebar.title("ğŸ‘‹ Sentiment Analysis ğŸ“„")
st.markdown(
    """
    <style>
    .centered-title {
        text-align: center; /* CÄƒn giá»¯a */
        font-family: 'Arial', sans-serif; /* Äá»•i font chá»¯ (hoáº·c thay báº±ng font khÃ¡c) */
        font-size: 4em; /* KÃ­ch thÆ°á»›c chá»¯ */
        font-weight: bold; /* Äáº­m chá»¯ */
    }
    </style>
    <h1 class="centered-title">ğŸ‘‹ Sentiment Analysis! ğŸ‘‹</h1>
    """,
    unsafe_allow_html=True
)

# ÄÆ°á»ng link cá»§a áº£nh
image_url = 'https://tamancosmetics.vn/wp-content/uploads/2024/01/hasaki.png'
# Hiá»ƒn thá»‹ áº£nh tá»« Ä‘Æ°á»ng link
# CÄƒn giá»¯a áº£nh vÃ  thay Ä‘á»•i kÃ­ch thÆ°á»›c
st.markdown(
    f"""
    <div style="display: flex; justify-content: center;">
        <img src="{image_url}" width="900">
    </div>
    """, 
    unsafe_allow_html=True
)

st.markdown(
        """
        <style>
        .intro-paragraph {
            text-indent: 0px; /* Thá»¥t lá» Ä‘áº§u dÃ²ng */
            margin-left: 0px; /* Thá»¥t toÃ n bá»™ Ä‘oáº¡n vÄƒn vÃ o */
            font-size: 1.8em; /* KÃ­ch thÆ°á»›c chá»¯ */
            line-height: 1.5; /* Khoáº£ng cÃ¡ch dÃ²ng */
            text-align: justify; /* Canh Ä‘á»u Ä‘oáº¡n vÄƒn */
            font-style: italic; /* In nghiÃªng Ä‘oáº¡n vÄƒn */
        }
        </style>
        <p class="intro-paragraph">
        <strong>ğŸ“‰ Dá»¯ Ä‘oÃ¡n nhÃ£n</strong>
        </p>
        """,
        unsafe_allow_html=True)

st.sidebar.write("""#### ThÃ nh viÃªn thá»±c hiá»‡n:\n
                 Trang ThÆ° ÄÃ¬nh &
                 Nguyá»…n Quang Kháº£i""")
st.sidebar.write("""#### Giáº£ng viÃªn hÆ°á»›ng dáº«n:\n
                Khuáº¥t ThÃ¹y PhÆ°Æ¡ng""")
st.sidebar.write("""#### Thá»i gian thá»±c hiá»‡n: 7/12/2024""")

processor = VietnameseTextProcessor(
    emoji_dict=emoji_dict,
    teen_dict=teen_dict,
    wrong_lst=wrong_lst,
    english_dict=english_dict,
    stopwords_lst=stopwords_lst
)
flag = False
lines = None

# Hiá»ƒn thá»‹ widget radio
type = st.radio("Táº£i bÃ¬nh luáº­n lÃªn hay Nháº­p liá»‡u?", options=("Táº£i lÃªn", "Nháº­p bÃ¬nh luáº­n"))
if type=="Táº£i lÃªn":
    # Upload file
    uploaded_file_1 = st.file_uploader("Chá»n file", type=['txt', 'csv', 'xlsx'])
    if uploaded_file_1 is not None:
        # Kiá»ƒm tra loáº¡i file vÃ  Ä‘á»c tÆ°Æ¡ng á»©ng
        if uploaded_file_1.name.endswith('.csv') or uploaded_file_1.name.endswith('.txt'):
            # Äá»c file CSV hoáº·c TXT, bá» qua cÃ¡c dÃ²ng cÃ³ lá»—i
            lines = pd.read_csv(uploaded_file_1, header=None, sep=';')
        elif uploaded_file_1.name.endswith('.xlsx'):
            # Äá»c file Excel
            lines = pd.read_excel(uploaded_file_1, header=None)
        # Chá»‰ láº¥y cá»™t Ä‘áº§u tiÃªn
        lines = lines[0]  
        # Táº¡o DataFrame má»›i vÃ  Ä‘á»•i tÃªn cá»™t thÃ nh 'noi_dung_binh_luan'
        du_doan = pd.DataFrame(lines)
        du_doan.columns = ['noi_dung_binh_luan']
        # Hiá»ƒn thá»‹ DataFrame má»›i
        st.markdown(
                f"""
                <style>
                .intro-paragraph {{
                    text-indent: 0px; /* Thá»¥t lá» Ä‘áº§u dÃ²ng */
                    margin-left: 0px; /* Thá»¥t toÃ n bá»™ Ä‘oáº¡n vÄƒn vÃ o */
                    font-size: 1.5em; /* KÃ­ch thÆ°á»›c chá»¯ */
                    line-height: 1.5; /* Khoáº£ng cÃ¡ch dÃ²ng */
                    text-align: justify; /* Canh Ä‘á»u Ä‘oáº¡n vÄƒn */
                    font-style: italic; /* In nghiÃªng Ä‘oáº¡n vÄƒn */
                }}
                </style>
                <p class="intro-paragraph">
                <strong>ğŸ’¬ Ná»™i dung bÃ¬nh luáº­n:</strong>
                </p>
                """,
                unsafe_allow_html=True)
        st.dataframe(du_doan)

        st.markdown(
            f"""
            <style>
            .intro-paragraph {{
                text-indent: 0px; /* Thá»¥t lá» Ä‘áº§u dÃ²ng */
                margin-left: 0px; /* Thá»¥t toÃ n bá»™ Ä‘oáº¡n vÄƒn vÃ o */
                font-size: 0.5em; /* KÃ­ch thÆ°á»›c chá»¯ nhá» */
                line-height: 1; /* Khoáº£ng cÃ¡ch dÃ²ng */
                text-align: center; /* Canh giá»¯a Ä‘oáº¡n vÄƒn */
                font-style: italic; /* In nghiÃªng Ä‘oáº¡n vÄƒn */
            }}
            </style>
            <p class="intro-paragraph">
            â³â³â³  Äang xá»­ lÃ½  â³â³â³
            </p>
            """,
            unsafe_allow_html=True)
        # LÆ°u Ã½: Cáº§n cung cáº¥p cÃ¡c tham sá»‘ nhÆ° processor, positive_words, negative_words, positive_emojis, negative_emojis.
        df_processed = du_doan['noi_dung_binh_luan'].apply(
            lambda x: preprocess_sentiment_text(x, processor, positive_words, negative_words, positive_emojis, negative_emojis)
        )
        # Merging káº¿t quáº£ vÃ o má»™t DataFrame duy nháº¥t
        du_doan = pd.concat(df_processed.tolist(), ignore_index=True)
        st.dataframe(du_doan)

        du_doan_combined = x_with_count_vectorizer_model(du_doan, model_path='saved_models/count_vectorizer_model.pkl')

        loaded_model = joblib.load('saved_models/Random_Forest_Classifier.pkl', mmap_mode='r')

        # Dá»± Ä‘oÃ¡n nhÃ£n
        predictions = loaded_model.predict(du_doan_combined)
        st.markdown(
                f"""
                <style>
                .intro-paragraph {{
                    text-indent: 0px; /* Thá»¥t lá» Ä‘áº§u dÃ²ng */
                    margin-left: 0px; /* Thá»¥t toÃ n bá»™ Ä‘oáº¡n vÄƒn vÃ o */
                    font-size: 1.5em; /* KÃ­ch thÆ°á»›c chá»¯ */
                    line-height: 1.5; /* Khoáº£ng cÃ¡ch dÃ²ng */
                    text-align: justify; /* Canh Ä‘á»u Ä‘oáº¡n vÄƒn */
                    font-style: italic; /* In nghiÃªng Ä‘oáº¡n vÄƒn */
                }}
                </style>
                <p class="intro-paragraph">
                <strong>ğŸ” Dá»± Ä‘oÃ¡n lÃ  nhÃ£n:</strong> {predictions}
                </p>
                """,
                unsafe_allow_html=True)

        # Dá»± Ä‘oÃ¡n xÃ¡c suáº¥t
        probabilities = loaded_model.predict_proba(du_doan_combined)

        # In xÃ¡c suáº¥t theo tá»«ng máº«u
        st.markdown(
                f"""
                <style>
                .intro-paragraph {{
                    text-indent: 0px; /* Thá»¥t lá» Ä‘áº§u dÃ²ng */
                    margin-left: 0px; /* Thá»¥t toÃ n bá»™ Ä‘oáº¡n vÄƒn vÃ o */
                    font-size: 1.5em; /* KÃ­ch thÆ°á»›c chá»¯ */
                    line-height: 1.5; /* Khoáº£ng cÃ¡ch dÃ²ng */
                    text-align: justify; /* Canh Ä‘á»u Ä‘oáº¡n vÄƒn */
                    font-style: italic; /* In nghiÃªng Ä‘oáº¡n vÄƒn */
                }}
                </style>
                <p class="intro-paragraph">
                <strong>ğŸ§® XÃ¡c xuáº¥t cá»§a cÃ¡c nhÃ£n:</strong>
                </p>
                """,
                unsafe_allow_html=True)
        class_labels = loaded_model.classes_
        prob_df = pd.DataFrame(probabilities, columns=class_labels)
        result = pd.merge(du_doan['noi_dung_binh_luan'], prob_df, left_index=True, right_index=True)
        def highlight_max_in_row(row):
            # So sÃ¡nh tá»« cá»™t thá»© 2 trá»Ÿ Ä‘i vÃ  Ã¡p dá»¥ng mÃ u vÃ ng cho giÃ¡ trá»‹ lá»›n nháº¥t
            return ['background-color: yellow' if v == row[1:].max() else '' for v in row]
        st.dataframe(result.style.apply(highlight_max_in_row, axis=1))


if type=="Nháº­p bÃ¬nh luáº­n":        
    text = st.text_area(label="Input your content:")
    if text!="":
        flag = True

        st.markdown(
            f"""
            <style>
            .intro-paragraph {{
                text-indent: 0px; /* Thá»¥t lá» Ä‘áº§u dÃ²ng */
                margin-left: 0px; /* Thá»¥t toÃ n bá»™ Ä‘oáº¡n vÄƒn vÃ o */
                font-size: 0.5em; /* KÃ­ch thÆ°á»›c chá»¯ nhá» */
                line-height: 1; /* Khoáº£ng cÃ¡ch dÃ²ng */
                text-align: center; /* Canh giá»¯a Ä‘oáº¡n vÄƒn */
                font-style: italic; /* In nghiÃªng Ä‘oáº¡n vÄƒn */
            }}
            </style>
            <p class="intro-paragraph">
            â³â³â³  Äang xá»­ lÃ½  â³â³â³
            </p>
            """,
            unsafe_allow_html=True)
        
        du_doan = preprocess_sentiment_text(text, processor, positive_words, negative_words, positive_emojis, negative_emojis)

        st.dataframe(du_doan)

        du_doan_combined = x_with_count_vectorizer_model(du_doan, model_path='saved_models/count_vectorizer_model.pkl')
        loaded_model = joblib.load('saved_models/Random_Forest_Classifier.pkl', mmap_mode='r')

        st.markdown(
                f"""
                <style>
                .intro-paragraph {{
                    text-indent: 0px; /* Thá»¥t lá» Ä‘áº§u dÃ²ng */
                    margin-left: 0px; /* Thá»¥t toÃ n bá»™ Ä‘oáº¡n vÄƒn vÃ o */
                    font-size: 1.5em; /* KÃ­ch thÆ°á»›c chá»¯ */
                    line-height: 1.5; /* Khoáº£ng cÃ¡ch dÃ²ng */
                    text-align: justify; /* Canh Ä‘á»u Ä‘oáº¡n vÄƒn */
                    font-style: italic; /* In nghiÃªng Ä‘oáº¡n vÄƒn */
                }}
                </style>
                <p class="intro-paragraph">
                <strong>ğŸ’¬ Ná»™i dung bÃ¬nh luáº­n:</strong> {text}
                </p>
                """,
                unsafe_allow_html=True)

        # Dá»± Ä‘oÃ¡n nhÃ£n
        predictions = loaded_model.predict(du_doan_combined)
        st.markdown(
                f"""
                <style>
                .intro-paragraph {{
                    text-indent: 0px; /* Thá»¥t lá» Ä‘áº§u dÃ²ng */
                    margin-left: 0px; /* Thá»¥t toÃ n bá»™ Ä‘oáº¡n vÄƒn vÃ o */
                    font-size: 1.5em; /* KÃ­ch thÆ°á»›c chá»¯ */
                    line-height: 1.5; /* Khoáº£ng cÃ¡ch dÃ²ng */
                    text-align: justify; /* Canh Ä‘á»u Ä‘oáº¡n vÄƒn */
                    font-style: italic; /* In nghiÃªng Ä‘oáº¡n vÄƒn */
                }}
                </style>
                <p class="intro-paragraph">
                <strong>ğŸ” Dá»± Ä‘oÃ¡n lÃ  nhÃ£n:</strong> {predictions}
                </p>
                """,
                unsafe_allow_html=True)
        
        # Dá»± Ä‘oÃ¡n xÃ¡c suáº¥t
        probabilities = loaded_model.predict_proba(du_doan_combined)

        # In xÃ¡c suáº¥t theo tá»«ng máº«u
        st.markdown(
                f"""
                <style>
                .intro-paragraph {{
                    text-indent: 0px; /* Thá»¥t lá» Ä‘áº§u dÃ²ng */
                    margin-left: 0px; /* Thá»¥t toÃ n bá»™ Ä‘oáº¡n vÄƒn vÃ o */
                    font-size: 1.5em; /* KÃ­ch thÆ°á»›c chá»¯ */
                    line-height: 1.5; /* Khoáº£ng cÃ¡ch dÃ²ng */
                    text-align: justify; /* Canh Ä‘á»u Ä‘oáº¡n vÄƒn */
                    font-style: italic; /* In nghiÃªng Ä‘oáº¡n vÄƒn */
                }}
                </style>
                <p class="intro-paragraph">
                <strong>ğŸ§® XÃ¡c xuáº¥t cá»§a cÃ¡c nhÃ£n:</strong>
                </p>
                """,
                unsafe_allow_html=True)
        class_labels = loaded_model.classes_
        prob_df = pd.DataFrame(probabilities, columns=class_labels)
        prob_df["Predicted Label"] = predictions

        def highlight_max_in_row(row):
            styles = ['background-color: yellow' if v == row[:-1].max() else '' for v in row[:-1]]  # KhÃ´ng highlight cá»™t nhÃ£n
            styles.append('font-weight: bold; color: blue')  # Nháº¥n máº¡nh cá»™t "Predicted Label"
            return styles
        styled_df = prob_df.style.apply(highlight_max_in_row, axis=1)
        st.dataframe(styled_df)




    